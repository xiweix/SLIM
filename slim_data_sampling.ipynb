{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973aa50b-72b2-4462-bad1-001b05e95b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from os.path import join as oj\n",
    "def add_path(path):\n",
    "    if path not in sys.path:\n",
    "        sys.path.insert(0, path)\n",
    "\n",
    "add_path(os.path.abspath('..'))\n",
    "from pycls.datasets.sampler import IndexedSequentialSampler\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "from utils.analysis import get_attention_new, attention_annotation_new, faiss_k_means, get_time, sample_from_group, compute_optimal_clusters, plot_clusters\n",
    "from utils.utils import compute_sampling_probability\n",
    "from utils.custom_data import CustomDataset\n",
    "\n",
    "# ANSI escape codes\n",
    "RED = \"\\033[31m\"\n",
    "BLUE = \"\\033[34m\"\n",
    "RESET = \"\\033[0m\"\n",
    "\n",
    "random_seed = 0 ### for reproductivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdb350f",
   "metadata": {},
   "source": [
    "### Load the dataset and reference model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5c72bf-70f6-4863-af8d-25b854032458",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'waterbirds'\n",
    "cur_exp = 'sol_1' ### sol_1 means using training set (compare slim_train) ### sol_2 means using validation set (slim_val)\n",
    "\n",
    "dataset = 'waterbirds'\n",
    "base_dir = \"[waterbird-data-dir]\"\n",
    "model_path = '[trained-model-path].pt'\n",
    "out_dir = \"../outputs/waterbirds\"\n",
    "\n",
    "data_flag = 'train' ### train or val\n",
    "img_dir = f'{base_dir}/waterbird_complete95_forest2water2'\n",
    "\n",
    "total_budget = 120\n",
    "att_budget_dict={0: 20, 1: 20}\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "model = getattr(models, 'resnet50')(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "data_transformer = transforms.Compose([transforms.ToTensor(),\n",
    "                                        transforms.Normalize(mean=mean, std=std),\n",
    "                                        transforms.Resize((224, 224))\n",
    "                                        ])\n",
    "\n",
    "# Data\n",
    "trainset = CustomDataset(basedir=base_dir, split=\"train\", transform=data_transformer, attention=True)\n",
    "valset = CustomDataset(basedir=base_dir, split=\"val\", transform=data_transformer, attention=True)\n",
    "dataset = trainset\n",
    "total_df = trainset.medadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c9de1c-0070-4e56-8f05-21c6e1c76bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(model_path, map_location=\"cpu\"))\n",
    "DEVICE=\"cuda:0\"\n",
    "model.cuda(DEVICE)\n",
    "all_true_idx_set = np.array(range(len(dataset)))\n",
    "subsetSampler = IndexedSequentialSampler(all_true_idx_set)\n",
    "loader = DataLoader(dataset=dataset, batch_size=1, sampler=subsetSampler, shuffle=False)\n",
    "print(\"data size: \", len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e56e461",
   "metadata": {},
   "source": [
    "### Generate GradCAM and required feature representations\n",
    "##### GradCAM and feature vector generation-related steps can be skipped if we already have them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c11f35-80ae-4851-99f2-f9a33d7f97b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_file_name_list = dataset.filename_array.tolist()\n",
    "img_name_list = [img_file_name_list[i].split('/')[-1] for i in range(len(img_file_name_list))]\n",
    "auto_att_scores, all_entropy_scores, total_prediction, total_label, img_names, img_paths, att_rep_embeddings, all_cam_masks, wfv, op_wfv = get_attention_new(model, loader, img_dir, out_dir, data_flag, dataset_name, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef26f21-2be0-4289-9ba2-0f5cb7eacd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_label = dataset.y_array\n",
    "total_place = dataset.p_array\n",
    "meta_info_str = np.array([f\"label_{total_label[i]}.place_{total_place[i]}\" for i in range(len(total_label))])\n",
    "meta_subgroup_array = (total_label * 2 + total_place).astype('int')\n",
    "confusion_matrix_str = []\n",
    "for prediction, label in zip(total_prediction, total_label):\n",
    "    if prediction == label:\n",
    "        if prediction == 1:  # Assuming 1 is the positive class\n",
    "            confusion_matrix_str.append(\"TP\")  # True Positive\n",
    "        else:\n",
    "            confusion_matrix_str.append(\"TN\")  # True Negative\n",
    "    else:\n",
    "        if prediction == 1:\n",
    "            confusion_matrix_str.append(\"FP\")  # False Positive\n",
    "        else:\n",
    "            confusion_matrix_str.append(\"FN\")  # False Negative\n",
    "confusion_matrix_str = np.array(confusion_matrix_str)\n",
    "meta_colors = ['steelblue', 'gold', 'crimson', 'forestgreen']\n",
    "unique_meta_groups = sorted(list(set(meta_info_str)))\n",
    "meta_to_color = dict(zip(unique_meta_groups, meta_colors))\n",
    "cm_colors = ['deepskyblue', 'tomato', 'mediumorchid', 'darkkhaki']\n",
    "unique_cm_groups = sorted(list(set(confusion_matrix_str)))\n",
    "cm_to_color = dict(zip(unique_cm_groups, cm_colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6934c67-e2b4-449c-bc39-b53a7b1fd5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.scatter(att_rep_embeddings[:, 0], att_rep_embeddings[:, 1], s=6)\n",
    "plt.title(f\"att_rep_embeddings ({len(att_rep_embeddings)})\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0866d7c",
   "metadata": {},
   "source": [
    "### An easy (less visual appealing) way for human annotation\n",
    "##### Sample data and a human can provide binary annotation results through text input\n",
    "Alternative: an interactive interface showing one image at a time with a binary selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da953fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_true_index = all_true_idx_set\n",
    "current_true_index_name = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aaf1ac",
   "metadata": {},
   "source": [
    "#### Below cell conducts data sampling for annotation, and prompts text: \n",
    "* \"Image grid xxx that require attention annotation is saved at: xxx.\"\n",
    "* \"Annotate the attention correctness of each image in order (0: wrong; 1: correct; -1: not sure).\"\n",
    "* \"Please separate by ', '\"\n",
    "\n",
    "A human can then provide annotations via text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e87bb1-3b7c-49c7-aa71-c813b250808d",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_annotation_flag = f'{cur_exp}_' + current_true_index_name\n",
    "print(\"att_budget_dict: \", att_budget_dict)\n",
    "print(\"att_annotation_flag: \", att_annotation_flag)\n",
    "if att_annotation_flag is not None:\n",
    "    annotated_att_scores = attention_annotation_new(auto_att_scores[current_true_index], img_names[current_true_index], total_label[current_true_index], total_prediction[current_true_index], img_paths[current_true_index], att_rep_embeddings[current_true_index], out_dir, att_annotation_flag, att_budget_dict=att_budget_dict, gamma=[10, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e3c588-5f3e-44f5-a01b-6a90acbd9182",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use automatic/annotated attention scores\n",
    "all_att_scores = annotated_att_scores\n",
    "total_wfv_embeddings = np.load(oj(out_dir, f'{data_flag}_total_weighted_fv_embedding.npy'))\n",
    "total_op_wfv_embeddings = np.load(oj(out_dir, f'{data_flag}_opposite_weighted_fv_embedding.npy'))\n",
    "wfv = np.load(oj(out_dir, f'{data_flag}_total_weighted_feature_vector.npy'))\n",
    "op_wfv = np.load(oj(out_dir, f'{data_flag}_opposite_weighted_feature_vector.npy'))\n",
    "print(\"total_wfv_embeddings_x: \", np.min(total_wfv_embeddings[:, 0]), np.max(total_wfv_embeddings[:, 0]))\n",
    "print(\"total_wfv_embeddings_y: \", np.min(total_wfv_embeddings[:, 1]), np.max(total_wfv_embeddings[:, 1]))\n",
    "print(\"total_op_wfv_embeddings_x: \", np.min(total_op_wfv_embeddings[:, 0]), np.max(total_op_wfv_embeddings[:, 0]))\n",
    "print(\"total_op_wfv_embeddings_y: \", np.min(total_op_wfv_embeddings[:, 1]), np.max(total_op_wfv_embeddings[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e44e811-6174-4f03-8f4d-2034824e19c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_index = np.where(all_att_scores > np.median(all_att_scores))[0]\n",
    "print(all_att_scores.shape, all_entropy_scores.shape, temp_index.shape)\n",
    "true_index = current_true_index[temp_index]\n",
    "current_true_index_name += '_higher_attention'\n",
    "print(true_index.shape)\n",
    "att_annotation_flag = f'{cur_exp}_' + current_true_index_name\n",
    "print(att_annotation_flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48989c4",
   "metadata": {},
   "source": [
    "#### Some additional visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec92f832-d46a-4a3f-ba8c-e91c3068d33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wfv[true_index].shape, total_wfv_embeddings[true_index].shape)\n",
    "group_colors = np.array([meta_to_color[group] for group in meta_info_str[true_index]])\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.scatter(total_wfv_embeddings[true_index, 0], total_wfv_embeddings[true_index, 1], c=group_colors, s=6)\n",
    "for group in unique_meta_groups:\n",
    "    plt.scatter([], [], color=meta_to_color[group], label=group)\n",
    "plt.legend(title='Meta Subgroups')\n",
    "plt.show()\n",
    "\n",
    "group_colors = np.array([cm_to_color[group] for group in confusion_matrix_str[true_index]])\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.scatter(total_wfv_embeddings[true_index, 0], total_wfv_embeddings[true_index, 1], c=group_colors, s=6)\n",
    "for group in unique_cm_groups:\n",
    "    plt.scatter([], [], color=cm_to_color[group], label=group)\n",
    "plt.legend(title='Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91615cb7-d6ba-431c-9abd-8d0281a12342",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wfv[true_index].shape, total_wfv_embeddings[true_index].shape)\n",
    "group_colors = np.array([meta_to_color[group] for group in meta_info_str[true_index]])\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.scatter(total_op_wfv_embeddings[true_index, 0], total_op_wfv_embeddings[true_index, 1], c=group_colors, s=6)\n",
    "for group in unique_meta_groups:\n",
    "    plt.scatter([], [], color=meta_to_color[group], label=group)\n",
    "plt.legend(title='Meta Subgroups')\n",
    "plt.show()\n",
    "\n",
    "group_colors = np.array([cm_to_color[group] for group in confusion_matrix_str[true_index]])\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.scatter(total_op_wfv_embeddings[true_index, 0], total_op_wfv_embeddings[true_index, 1], c=group_colors, s=6)\n",
    "for group in unique_cm_groups:\n",
    "    plt.scatter([], [], color=cm_to_color[group], label=group)\n",
    "plt.legend(title='Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9537a5a5",
   "metadata": {},
   "source": [
    "### Data subset curation\n",
    "#### Based on spuriousness score propagated according to human annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f372bdcf-92de-4ddf-b331-aa2b5a66642c",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time_str = get_time()\n",
    "current_out_dir = oj(out_dir, f\"{cur_exp}_{current_time_str}_sampling\")\n",
    "\n",
    "constructed_set_name = 'constructed_set'\n",
    "print(constructed_set_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09e827c-93dd-4be3-8c4a-8f9e419781bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(current_out_dir, exist_ok=True)\n",
    "print(f\"save results in {current_out_dir}\")\n",
    "\n",
    "current_wfv_embeddings = total_wfv_embeddings[true_index]\n",
    "current_op_wfv_embeddings = total_op_wfv_embeddings[true_index]\n",
    "\n",
    "current_wfv = wfv[true_index]\n",
    "current_op_wfv = op_wfv[true_index]\n",
    "current_label = total_label[true_index]\n",
    "print(true_index.shape)\n",
    "print(current_wfv.shape, current_wfv_embeddings.shape)\n",
    "print(current_op_wfv.shape, current_op_wfv_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef82af6-cc40-4ec3-ae0c-cf6323883f5a",
   "metadata": {},
   "source": [
    "#### Find the optimnal n_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5255feeb-5329-404f-9668-51570306cdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_options = [2,3,4,5]\n",
    "eval_metric = 'elbow' ### ['silhouette', 'elbow', 'davies_bouldin']\n",
    "optimal_wfv_clusters, best_wfv_score, optimal_wfv_cluster_labels = compute_optimal_clusters(current_wfv_embeddings, cluster_options, eval_metric)\n",
    "print(f\"Org - Optimal number of clusters: {optimal_wfv_clusters} with a {eval_metric} score of {best_wfv_score}\")\n",
    "plot_clusters(current_wfv_embeddings, optimal_wfv_cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db3adcd-5ba4-445e-8b5f-79efda0ba167",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_options = [2,3,4,5]\n",
    "optimal_op_wfv_clusters, best_op_wfv_score, optimal_op_wfv_cluster_labels = compute_optimal_clusters(current_op_wfv_embeddings, cluster_options, eval_metric)\n",
    "print(f\"Rev - Optimal number of clusters: {optimal_op_wfv_clusters} with a {eval_metric} score of {best_op_wfv_score}\")\n",
    "plot_clusters(current_op_wfv_embeddings, optimal_op_wfv_cluster_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d4b62b-440c-46af-86ea-b1a9d761235a",
   "metadata": {},
   "source": [
    "### Apply clustering on feature representation and reverse feature representation spaces, respectively\n",
    "##### Use the optimal n_clusters for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e0e329-4975-416c-8cc4-b4d56e4af9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_1_kmeans_model, cluster_1_labels = faiss_k_means(current_wfv_embeddings, optimal_wfv_clusters)\n",
    "cluster_2_kmeans_model, cluster_2_labels = faiss_k_means(current_op_wfv_embeddings, optimal_op_wfv_clusters)\n",
    "plot_clusters(current_wfv_embeddings, cluster_1_labels)\n",
    "plot_clusters(current_op_wfv_embeddings, cluster_2_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aba3bfd-67da-4c56-bf5c-e0898e1e6efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cluster_labels = (cluster_1_labels * optimal_op_wfv_clusters + cluster_2_labels).astype('int')\n",
    "final_cluster_label_count = len(np.unique(final_cluster_labels))\n",
    "final_cluster_label_counter = Counter(final_cluster_labels)\n",
    "print(\"true_index\", true_index.shape)\n",
    "print(\"cluster_1_labels\", cluster_1_labels.shape, Counter(cluster_1_labels))\n",
    "print(\"cluster_2_labels\", cluster_2_labels.shape, Counter(cluster_2_labels))\n",
    "print(\"final_cluster_labels\", final_cluster_labels.shape, Counter(final_cluster_labels))\n",
    "print(\"current_wfv_embeddings\", current_wfv_embeddings.shape)\n",
    "print(\"current_op_wfv_embeddings\", current_op_wfv_embeddings.shape)\n",
    "np.save(oj(current_out_dir, 'true_index.npy'), true_index)\n",
    "np.save(oj(current_out_dir, 'cluster_1_labels.npy'), cluster_1_labels)\n",
    "np.save(oj(current_out_dir, 'cluster_2_labels.npy'), cluster_2_labels)\n",
    "np.save(oj(current_out_dir, 'final_cluster_labels.npy'), final_cluster_labels)\n",
    "np.save(oj(current_out_dir, 'current_wfv_embeddings.npy'), current_wfv_embeddings)\n",
    "np.save(oj(current_out_dir, 'current_op_wfv_embeddings.npy'), current_op_wfv_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b042bc7-5a71-4b02-bc1d-b74d2ea0c8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "org_size = [final_cluster_label_counter[i] for i in range(final_cluster_label_count)]\n",
    "sampling_weights = np.zeros(optimal_op_wfv_clusters * optimal_op_wfv_clusters)\n",
    "cluster_1_prob = compute_sampling_probability(current_wfv_embeddings, cluster_1_labels)\n",
    "print(f\"Cluster 1 sampling probability: {cluster_1_prob}\")\n",
    "sampling_size = np.zeros(optimal_op_wfv_clusters * optimal_op_wfv_clusters, dtype=int)\n",
    "t_budget = 0\n",
    "for i in range(optimal_op_wfv_clusters):\n",
    "    temp_index = np.where(cluster_1_labels == i)[0]\n",
    "    cluster_2_prob = compute_sampling_probability(current_op_wfv_embeddings[temp_index], cluster_2_labels[temp_index])\n",
    "    print(f\"Cluster 1 label: {i} | Corresponding cluster 2 sampling probability: {cluster_2_prob}\")\n",
    "    for j in range(optimal_op_wfv_clusters):\n",
    "        final_label = i * optimal_op_wfv_clusters + j\n",
    "        final_label_instance_count = final_cluster_label_counter[final_label]\n",
    "        final_probability = cluster_1_prob[i] * cluster_2_prob[j]  ####\n",
    "        count = int(final_probability * total_budget)\n",
    "        print(f\"final cluster: {final_label} (clustering_1 label: {i} | clustering_2 label: {j}) | count: {final_label_instance_count} | sampling power({final_probability}) * budget({total_budget}={count})\")\n",
    "        sampling_weights[final_label] = final_probability\n",
    "        sampling_size[final_label] = count\n",
    "        index = np.where(final_cluster_labels==final_label)[0]\n",
    "        t_index = true_index[index]\n",
    "sampling_weights_sum = np.sum(sampling_weights)\n",
    "sampling_weights = [w / sampling_weights_sum for w in sampling_weights]\n",
    "sampling_size = [int(total_budget * w) for w in sampling_weights]\n",
    "print(\"sampling_weights: \", sampling_weights, np.sum(sampling_weights))\n",
    "\n",
    "print(\"org_size: \", org_size)\n",
    "print(\"sampling_size: \", sampling_size)\n",
    "print(\"total sampling: \", np.sum(sampling_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620380fe-45a8-4779-9d67-96c9e5d2d0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_final_clusters = np.max(final_cluster_labels) + 1 \n",
    "print(\"n_final_clusters: \", n_final_clusters)\n",
    "cluster_group_idx = []\n",
    "for cur_final_cluster_label in range(n_final_clusters):\n",
    "    temp = np.where(final_cluster_labels == cur_final_cluster_label)[0]\n",
    "    if len(temp) != 0:\n",
    "        cluster_group_idx.append(temp)\n",
    "balanced_index = []\n",
    "for group_idx, t_sampling_size in zip(cluster_group_idx, sampling_size):\n",
    "    t_sampling_size = int(t_sampling_size)\n",
    "    sampled_group_idx = sample_from_group(group_idx, t_sampling_size, group_vector=current_op_wfv_embeddings[group_idx], seed=random_seed)\n",
    "    if len(sampled_group_idx) > len(group_idx):\n",
    "        print(f\"{BLUE}sample {len(sampled_group_idx)} from {len(group_idx)}{RESET}. Duplicated samples: {len(sampled_group_idx) - len(group_idx)}\")\n",
    "    else:\n",
    "        print(f\"{BLUE}sample {len(sampled_group_idx)} from {len(group_idx)}{RESET}\")\n",
    "    balanced_index.append(sampled_group_idx)\n",
    "balanced_index = np.concatenate(balanced_index)\n",
    "constructed_set_name += f\"_{len(balanced_index)}\"\n",
    "print(\"constructed_set_name: \", constructed_set_name)\n",
    "print(f\"In total, sampled {len(balanced_index)} from {len(true_index)}: \")\n",
    "constructed_set = true_index[balanced_index]\n",
    "print(f'{RED}', len(constructed_set), Counter(meta_info_str[constructed_set]), f'{RESET}')\n",
    "remove_dup_constructed_set = set(constructed_set)\n",
    "remove_dup_constructed_set = np.array(list(remove_dup_constructed_set))\n",
    "print(f'{RED} Duplicated samples: ',len(constructed_set) - len(remove_dup_constructed_set), f'{RESET}')\n",
    "with open(os.path.join(current_out_dir, f\"{constructed_set_name}_info.txt\"), 'a') as f:\n",
    "    f.write(f\"random_seed: {random_seed}\\n\")\n",
    "    f.write(f\"sampling_weights: {sampling_weights}\\n\")\n",
    "    f.write(f\"sampling_size: {sampling_size}\\n\")\n",
    "    f.write(f\"Duplicated samples: {len(constructed_set) - len(remove_dup_constructed_set)}\\n\")\n",
    "    f.write(f\"true_index: {true_index.shape}\\n\")\n",
    "    f.write(f\"total_budget: {total_budget}\\n\")\n",
    "    f.write(f\"optimal_op_wfv_clusters: {optimal_op_wfv_clusters}\\n\")\n",
    "    f.write(f\"optimal_op_wfv_clusters: {optimal_op_wfv_clusters}\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e57d369-713e-4281-8754-ef7ff85e5f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n\\ncluster_1_labels {cluster_1_labels.shape}\")\n",
    "with open(os.path.join(current_out_dir, f\"{constructed_set_name}_info.txt\"), 'a') as f:\n",
    "    f.write(f\"\\n\\ncluster_1_labels {cluster_1_labels.shape}\\n\")\n",
    "cur_unique_groups = list(set(cluster_1_labels))\n",
    "for group in cur_unique_groups:\n",
    "    index = np.where(cluster_1_labels==group)[0]\n",
    "    t_index = true_index[index]\n",
    "    print(group, index.shape, '\\t', Counter(meta_info_str[t_index]), '\\t', Counter(confusion_matrix_str[t_index]))\n",
    "    sampled_in_current_subgroup = []\n",
    "    for i in constructed_set:\n",
    "        if i in t_index:\n",
    "            sampled_in_current_subgroup.append(i)\n",
    "    if len(sampled_in_current_subgroup) != 0:\n",
    "        sampled_in_current_subgroup = np.array(sampled_in_current_subgroup)\n",
    "        print(BLUE + '  sampled: ', sampled_in_current_subgroup.shape, Counter(meta_info_str[sampled_in_current_subgroup]), f'{RESET}')\n",
    "        print(BLUE + '\\t\\t\\t', Counter(confusion_matrix_str[sampled_in_current_subgroup]), f'{RESET}')\n",
    "        with open(os.path.join(current_out_dir, f\"{constructed_set_name}_info.txt\"), 'a') as f:\n",
    "            f.write(f\"{group}, {index.shape}\\n\")\n",
    "            f.write(f\"\\t{Counter(meta_info_str[t_index])}\\t{Counter(confusion_matrix_str[t_index])}\\n\")\n",
    "            f.write(f\"\\t{Counter(meta_info_str[sampled_in_current_subgroup])}\\t{Counter(confusion_matrix_str[sampled_in_current_subgroup])}\\n\")\n",
    "    else:\n",
    "        print(BLUE + '  sampled: ', len(sampled_in_current_subgroup), f'{RESET}')\n",
    "        with open(os.path.join(current_out_dir, f\"{constructed_set_name}_info.txt\"), 'a') as f:\n",
    "            f.write(f\"{group}, {index.shape} | sampled: {len(sampled_in_current_subgroup)}\\n\")\n",
    "\n",
    "print(f\"\\n\\ncluster_2_labels {cluster_2_labels.shape}\")\n",
    "with open(os.path.join(current_out_dir, f\"{constructed_set_name}_info.txt\"), 'a') as f:\n",
    "    f.write(f\"\\n\\ncluster_2_labels {cluster_2_labels.shape}\\n\")\n",
    "cur_unique_groups = list(set(cluster_2_labels))\n",
    "for group in cur_unique_groups:\n",
    "    index = np.where(cluster_2_labels==group)[0]\n",
    "    t_index = true_index[index]\n",
    "    print(group, index.shape, '\\t', Counter(meta_info_str[t_index]), '\\t', Counter(confusion_matrix_str[t_index]))\n",
    "    sampled_in_current_subgroup = []\n",
    "    for i in constructed_set:\n",
    "        if i in t_index:\n",
    "            sampled_in_current_subgroup.append(i)\n",
    "    if len(sampled_in_current_subgroup) != 0:\n",
    "        sampled_in_current_subgroup = np.array(sampled_in_current_subgroup)\n",
    "        print(BLUE + '  sampled: ', sampled_in_current_subgroup.shape, Counter(meta_info_str[sampled_in_current_subgroup]), f'{RESET}')\n",
    "        print(BLUE + '\\t\\t\\t', Counter(confusion_matrix_str[sampled_in_current_subgroup]), f'{RESET}')\n",
    "        with open(os.path.join(current_out_dir, f\"{constructed_set_name}_info.txt\"), 'a') as f:\n",
    "            f.write(f\"{group}, {index.shape}\\n\")\n",
    "            f.write(f\"\\t{Counter(meta_info_str[t_index])}\\t{Counter(confusion_matrix_str[t_index])}\\n\")\n",
    "            f.write(f\"\\t{Counter(meta_info_str[sampled_in_current_subgroup])}\\t{Counter(confusion_matrix_str[sampled_in_current_subgroup])}\\n\")\n",
    "    else:\n",
    "        print(BLUE + '  sampled: ', len(sampled_in_current_subgroup), f'{RESET}')\n",
    "        with open(os.path.join(current_out_dir, f\"{constructed_set_name}_info.txt\"), 'a') as f:\n",
    "            f.write(f\"{group}, {index.shape} | sampled: {len(sampled_in_current_subgroup)}\\n\")\n",
    "\n",
    "print(f\"\\n\\nfinal {len(final_cluster_labels)}\")\n",
    "with open(os.path.join(current_out_dir, f\"{constructed_set_name}_info.txt\"), 'a') as f:\n",
    "    f.write(f\"\\n\\nfinal\\n\")\n",
    "cur_unique_groups = list(set(final_cluster_labels))\n",
    "for group in cur_unique_groups:\n",
    "    index = np.where(final_cluster_labels==group)[0]\n",
    "    t_index = true_index[index]\n",
    "    print(group, index.shape, '\\t', Counter(meta_info_str[t_index]), '\\t', Counter(confusion_matrix_str[t_index]))\n",
    "    sampled_in_current_subgroup = []\n",
    "    for i in constructed_set:\n",
    "        if i in t_index:\n",
    "            sampled_in_current_subgroup.append(i)\n",
    "    if len(sampled_in_current_subgroup) != 0:\n",
    "        sampled_in_current_subgroup = np.array(sampled_in_current_subgroup)\n",
    "        print(BLUE + '  sampled: ', sampled_in_current_subgroup.shape, Counter(meta_info_str[sampled_in_current_subgroup]), f'{RESET}')\n",
    "        print(BLUE + '\\t\\t\\t', Counter(confusion_matrix_str[sampled_in_current_subgroup]), f'{RESET}')\n",
    "        with open(os.path.join(current_out_dir, f\"{constructed_set_name}_info.txt\"), 'a') as f:\n",
    "            f.write(f\"{group}, {index.shape}\\n\")\n",
    "            f.write(f\"\\t{Counter(meta_info_str[t_index])}\\t{Counter(confusion_matrix_str[t_index])}\\n\")\n",
    "            f.write(f\"\\t{Counter(meta_info_str[sampled_in_current_subgroup])}\\t{Counter(confusion_matrix_str[sampled_in_current_subgroup])}\\n\")\n",
    "    else:\n",
    "        print(BLUE + '  sampled: ', len(sampled_in_current_subgroup), f'{RESET}')\n",
    "        with open(os.path.join(current_out_dir, f\"{constructed_set_name}_info.txt\"), 'a') as f:\n",
    "            f.write(f\"{group}, {index.shape} | sampled: {len(sampled_in_current_subgroup)}\\n\")\n",
    "\n",
    "constructed_set_path = os.path.join(current_out_dir, f\"{constructed_set_name}.npy\")\n",
    "np.save(constructed_set_path, constructed_set)\n",
    "\n",
    "label_counter = Counter(total_label[constructed_set])\n",
    "class_weight = {}\n",
    "for l in np.unique(total_label):\n",
    "    w = 1 / label_counter[l] / (1 / label_counter[0])\n",
    "    class_weight[l] = w\n",
    "    print(f'label: {l} | count: {label_counter[l]} | weight: {w}')\n",
    "print(\"\\n\", \"class weight: \",  class_weight)\n",
    "\n",
    "with open(os.path.join(current_out_dir, f\"{constructed_set_name}_info.txt\"), 'a') as f:\n",
    "    f.write(f\"Index of length {len(constructed_set)} saved at {constructed_set_path}\\n\")\n",
    "print(f\"Index of length {len(constructed_set)} saved at {constructed_set_path}\")\n",
    "with open(os.path.join(current_out_dir, f\"{constructed_set_name}_info.txt\"), 'a') as f:\n",
    "    f.write(f\"{constructed_set.shape}\\n\\n\")\n",
    "    f.write(f\"{Counter(meta_info_str[constructed_set])}\\n\\n\")\n",
    "    f.write(f\"{label_counter}\\n\\n\")\n",
    "    f.write(f\"{class_weight}\\n\\n\")\n",
    "    \n",
    "constructed_set_path, constructed_set.shape, Counter(meta_info_str[constructed_set]), class_weight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
